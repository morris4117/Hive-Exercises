[training@localhost ~]$ hive
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
Hive history file=/tmp/training/hive_job_log_training_202302141210_1628757870.txt
hive> create table userRatings1(userid int,movieid int,rating int,unixtime bigimcd ..                                                         
    > clear
    > [training@localhost ~]$ hive
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
Hive history file=/tmp/training/hive_job_log_training_202302141214_397389583.txt
hive> Create table userrating
    > (userid int,movieid int,rating int,unixtime bigint) Row format delimited
    > fields terminated by '\t'
    > stored as textfile;
OK
Time taken: 3.697 seconds
hive> load data inpath hiveworks/u.data
    > into table userratings;
FAILED: ParseException line 1:17 mismatched input 'hiveworks' expecting StringLiteral near 'inpath' in load statement

hive> into table userrating; 
FAILED: ParseException line 1:0 cannot recognize input near 'into' 'table' 'userrating'

hive> load data inpath 'hiveworks/u.data'
    > into table userrating;             
Loading data to table default.userrating
OK
Time taken: 0.725 seconds
hive> select * FROM userating limit 5;
FAILED: SemanticException [Error 10001]: Line 1:14 Table not found 'userating'
hive> select * FROM useratings limit 5;
FAILED: SemanticException [Error 10001]: Line 1:14 Table not found 'useratings'
hive> select * from useratings limit 5;
FAILED: SemanticException [Error 10001]: Line 1:14 Table not found 'useratings'
hive> select * from userating limit 5; 
FAILED: SemanticException [Error 10001]: Line 1:14 Table not found 'userating'
hive> select * from userrating limit 5; 
OK
196	242	3	881250949
186	302	3	891717742
22	377	1	878887116
244	51	2	880606923
166	346	1	886397596
Time taken: 0.411 seconds
hive> create table user(userid int
    > [training@localhoshadoop fs -ls combiner/output^C[training@localhost ~]$ hive
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
Hive history file=/tmp/training/hive_job_log_training_202302141317_992216291.txt
hive> create table user(useuser id | age | gender | occupation | zip code
    > create table user(useuser id | age | gender | occupation | zip code[training@localhost ~]$ ^C
[training@localhost ~]$ hive
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
Hive history file=/tmp/training/hive_job_log_training_202302141318_1046675128.txt
hive> create table user(string userid,int age,string gender,string ocupation,int zip code)row format delimited 
    > fields terminated by '|'
    > stored as textfile;
FAILED: ParseException line 1:18 mismatched input 'string' expecting Identifier near '(' in column specification

hive> create table user(string userid,int age,string gender,string ocupation,int                                                                                string userid,int age,string gender,string ocupation,int zip code)row format del                                                                                select * from useratings limit 5;[training@localhost ~]$           
[training@localhost ~]$ hive
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
Hive history file=/tmp/training/hive_job_log_training_202302141328_360449044.txt
hive> create table user(userid string,age int,gender string,ocupation string,zip int)row format delimited             
    > fields terminated by '|'                                                                            
    > stored as textfile;                                                                                 
OK
Time taken: 2.773 seconds
hive> load data local inpath '/home/training/training_materials/hive_and_pig/data/u.user' 
    > into table user
    > into table user;
FAILED: ParseException line 3:0 mismatched input 'into' expecting EOF near 'user'

hive> load data local inpath '/home/training/training_materials/hive_and_pig/data/u.user'
    > into table user;                                                                   
FAILED: SemanticException Line 1:23 Invalid path ''/home/training/training_materials/hive_and_pig/data/u.user'': No files matching path file:/home/training/training_materials/hive_and_pig/data/u.user
hive> load data local inpath "/home/training/training_materials/hive_and_pig/data/u.user" 
    > into table user;                                                                   
FAILED: SemanticException Line 1:23 Invalid path '"/home/training/training_materials/hive_and_pig/data/u.user"': No files matching path file:/home/training/training_materials/hive_and_pig/data/u.user
hive> load data local inpath '/home/training/hiveworkspace/ml-data/u.users'                               
    > into table user;                                                     
FAILED: SemanticException Line 1:23 Invalid path ''/home/training/hiveworkspace/ml-data/u.users'': No files matching path file:/home/training/hiveworkspace/ml-data/u.users
hive> load data local inpath '/home/training/hiveworkspace/ml-data/u.user' 
    > into table user;                                                    
Copying data from file:/home/training/hiveworkspace/ml-data/u.user
Copying file: file:/home/training/hiveworkspace/ml-data/u.user
Loading data to table default.user
OK
Time taken: 0.412 seconds
hive> select * from userrating join user on userrating.userid limit -5;
FAILED: ParseException line 1:62 mismatched input '-' expecting Number near 'limit' in limit clause

hive> select * from userrating join user on userrating.userid limit 5; 
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202302141142_0001, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202302141142_0001
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202302141142_0001
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-02-14 13:43:27,747 Stage-1 map = 0%,  reduce = 0%
2023-02-14 13:43:31,921 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.78 sec
2023-02-14 13:43:32,939 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.78 sec
2023-02-14 13:43:33,947 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.78 sec
2023-02-14 13:43:34,956 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.78 sec
2023-02-14 13:43:35,965 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.78 sec
2023-02-14 13:43:36,974 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.78 sec
2023-02-14 13:43:37,984 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.78 sec
2023-02-14 13:43:38,991 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.78 sec
2023-02-14 13:43:39,998 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:41,007 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:42,016 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:43,025 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:44,042 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:45,051 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:46,062 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:47,072 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:48,079 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:49,088 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:50,097 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:51,105 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:52,114 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:53,122 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 1.78 sec
2023-02-14 13:43:54,159 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.78 sec
MapReduce Total cumulative CPU time: 1 seconds 780 msec
Ended Job = job_202302141142_0001 with errors
Error during job, obtaining debugging information...
Examining task ID: task_202302141142_0001_m_000003 (and more) from job job_202302141142_0001

Task with the most failures(4): 
-----
Task ID:
  task_202302141142_0001_m_000000

URL:
  http://0.0.0.0:50030/taskdetails.jsp?jobid=job_202302141142_0001&tipid=task_202302141142_0001_m_000000
-----
Diagnostic Messages for this Task:
java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"userid":196,"movieid":242,"rating":3,"unixtime":881250949}
	at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:161)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:393)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:327)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"userid":196,"movieid":242,"rating":3,"unixtime":881250949}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:548)
	at org.apache.hadoop.hive

FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 1.78 sec   HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 1 seconds 780 msec
hive> select * from userrating join user on userrating.userid = user.userid limit 5;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202302141142_0002, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202302141142_0002
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202302141142_0002
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-02-14 13:46:04,453 Stage-1 map = 0%,  reduce = 0%
2023-02-14 13:46:08,484 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 2.05 sec
2023-02-14 13:46:09,492 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.07 sec
2023-02-14 13:46:10,499 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.07 sec
2023-02-14 13:46:11,508 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.07 sec
2023-02-14 13:46:12,519 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9.53 sec
2023-02-14 13:46:13,530 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9.53 sec
2023-02-14 13:46:14,547 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9.53 sec
MapReduce Total cumulative CPU time: 9 seconds 530 msec
Ended Job = job_202302141142_0002
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 9.53 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 9 seconds 530 msec
OK
1	270	5	888732827	1	24	M	technician	85711
1	133	4	876892818	1	24	M	technician	85711
1	29	1	878542869	1	24	M	technician	85711
1	179	3	875072370	1	24	M	technician	85711
1	237	2	875071749	1	24	M	technician	85711
Time taken: 13.84 seconds
hive> 